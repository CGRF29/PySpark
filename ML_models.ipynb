{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de modelos ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a local Spark cluster using all available cores, which will be accessible via a SparkSession object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('model_ML') \\\n",
    "                    .getOrCreate()\n",
    "# What version of Spark?\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/diabetes.csv',sep=',',header=True,inferSchema=True,nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 768 records.\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % df.count())\n",
    "# View the first five records\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pregnancies', 'int'), ('Glucose', 'int'), ('BloodPressure', 'int'), ('SkinThickness', 'int'), ('Insulin', 'int'), ('BMI', 'double'), ('DiabetesPedigreeFunction', 'double'), ('Age', 'int'), ('Outcome', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Check column data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records:  768\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of records: \", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records after removing the duplicate :  768\n"
     ]
    }
   ],
   "source": [
    "df=df.dropDuplicates()\n",
    "print(\"The number of records after removing the duplicate : \", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# Remove records with missing values \n",
    "df = df.dropna()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|            features|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "|          4|    129|           60|           12|    231|27.5|                   0.527| 31|      0|[4.0,129.0,60.0,1...|\n",
      "|          2|    105|           58|           40|     94|34.9|                   0.225| 25|      0|[2.0,105.0,58.0,4...|\n",
      "|          3|    129|           64|           29|    115|26.4|                   0.219| 28|      1|[3.0,129.0,64.0,2...|\n",
      "|          7|    136|           90|            0|      0|29.9|                    0.21| 50|      0|[7.0,136.0,90.0,0...|\n",
      "|          6|      0|           68|           41|      0|39.0|                   0.727| 41|      1|[6.0,0.0,68.0,41....|\n",
      "|         12|     84|           72|           31|      0|29.7|                   0.297| 46|      1|[12.0,84.0,72.0,3...|\n",
      "|          3|    103|           72|           30|    152|27.6|                    0.73| 27|      0|[3.0,103.0,72.0,3...|\n",
      "|          6|    165|           68|           26|    168|33.6|                   0.631| 49|      0|[6.0,165.0,68.0,2...|\n",
      "|         10|    125|           70|           26|    115|31.1|                   0.205| 41|      1|[10.0,125.0,70.0,...|\n",
      "|          3|    112|           74|           30|      0|31.6|                   0.197| 25|      1|[3.0,112.0,74.0,3...|\n",
      "|          5|    139|           80|           35|    160|31.6|                   0.361| 25|      1|[5.0,139.0,80.0,3...|\n",
      "|         12|    106|           80|            0|      0|23.6|                   0.137| 44|      0|[12.0,106.0,80.0,...|\n",
      "|          1|    119|           54|           13|     50|22.3|                   0.205| 24|      0|[1.0,119.0,54.0,1...|\n",
      "|          2|     81|           60|           22|      0|27.7|                    0.29| 25|      0|[2.0,81.0,60.0,22...|\n",
      "|          5|    112|           66|            0|      0|37.8|                   0.261| 41|      1|[5.0,112.0,66.0,0...|\n",
      "|         10|    179|           70|            0|      0|35.1|                     0.2| 37|      0|[10.0,179.0,70.0,...|\n",
      "|          5|     86|           68|           28|     71|30.2|                   0.364| 24|      0|[5.0,86.0,68.0,28...|\n",
      "|          0|     97|           64|           36|    100|36.8|                     0.6| 25|      0|[0.0,97.0,64.0,36...|\n",
      "|          0|    141|           84|           26|      0|32.4|                   0.433| 22|      0|[0.0,141.0,84.0,2...|\n",
      "|          2|     91|           62|            0|      0|27.3|                   0.525| 22|      0|[2.0,91.0,62.0,0....|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pregnancies\", \"Glucose\", \"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "assembled_df = assembler.transform(df)\n",
    "assembled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Ajustar el scaler en el DataFrame\n",
    "scalerModel = scaler.fit(assembled_df)\n",
    "\n",
    "# Transformar los datos\n",
    "scaledData = scalerModel.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      scaledFeatures|Outcome|\n",
      "+--------------------+-------+\n",
      "|[0.23529411764705...|      0|\n",
      "|[0.11764705882352...|      0|\n",
      "|[0.17647058823529...|      1|\n",
      "|[0.41176470588235...|      0|\n",
      "|[0.35294117647058...|      1|\n",
      "|[0.70588235294117...|      1|\n",
      "|[0.17647058823529...|      0|\n",
      "|[0.35294117647058...|      0|\n",
      "|[0.58823529411764...|      1|\n",
      "|[0.17647058823529...|      1|\n",
      "|[0.29411764705882...|      1|\n",
      "|[0.70588235294117...|      0|\n",
      "|[0.05882352941176...|      0|\n",
      "|[0.11764705882352...|      0|\n",
      "|[0.29411764705882...|      1|\n",
      "|[0.58823529411764...|      0|\n",
      "|[0.29411764705882...|      0|\n",
      "|[0.0,0.4874371859...|      0|\n",
      "|[0.0,0.7085427135...|      0|\n",
      "|[0.11764705882352...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select(\"scaledFeatures\",\"Outcome\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83984375\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "df_train, df_test = scaledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = df_train.count() / scaledData.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DT classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = tree.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------------------------------+\n",
      "|Outcome|prediction|probability                              |\n",
      "+-------+----------+-----------------------------------------+\n",
      "|0      |0.0       |[0.9915966386554622,0.008403361344537815]|\n",
      "|0      |0.0       |[0.8392857142857143,0.16071428571428573] |\n",
      "|0      |0.0       |[0.8392857142857143,0.16071428571428573] |\n",
      "|0      |0.0       |[0.9915966386554622,0.008403361344537815]|\n",
      "|0      |0.0       |[0.9915966386554622,0.008403361344537815]|\n",
      "|0      |0.0       |[0.9915966386554622,0.008403361344537815]|\n",
      "|0      |0.0       |[0.9915966386554622,0.008403361344537815]|\n",
      "|0      |0.0       |[0.8392857142857143,0.16071428571428573] |\n",
      "|0      |0.0       |[0.9714285714285714,0.02857142857142857] |\n",
      "|1      |0.0       |[0.8392857142857143,0.16071428571428573] |\n",
      "+-------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create predictions on test data\n",
    "prediction = tree_model.transform(df_test)\n",
    "prediction.select('Outcome', 'prediction', 'probability').show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of: True Negatives (TN) — prediction is negative & label is negative\n",
    "\n",
    "True Positives (TP) — prediction is positive & label is positive\n",
    "\n",
    "False Negatives (FN) — prediction is negative & label is positive\n",
    "\n",
    "False Positives (FP) — prediction is positive & label is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|Outcome|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|       0.0|   18|\n",
      "|      0|       0.0|   70|\n",
      "|      1|       1.0|   25|\n",
      "|      0|       1.0|   10|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('Outcome', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND Outcome = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND Outcome = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND Outcome != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND Outcome != prediction').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "eval_precision = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "eval_recall = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "eval_f1 = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7723577235772358\n",
      "Precision: 0.7954545454545454\n",
      "Recall: 0.875\n",
      "F1 Score: 0.7661038148843028\n",
      "AUC: 0.7723577235772358\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_accuracy.evaluate(prediction)\n",
    "precision = eval_precision.evaluate(prediction)\n",
    "recall = eval_recall.evaluate(prediction)\n",
    "f1score = eval_f1.evaluate(prediction)\n",
    "auc = eval_accuracy.evaluate(prediction)\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('F1 Score:',f1score)\n",
    "print('AUC:',auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|Outcome|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|       0.0|   18|\n",
      "|      0|       0.0|   71|\n",
      "|      1|       1.0|   25|\n",
      "|      0|       1.0|    9|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression(featuresCol=\"features\", labelCol=\"Outcome\").fit(df_train)\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(df_test)\n",
    "prediction.groupBy('Outcome', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7804878048780488\n",
      "Precision: 0.797752808988764\n",
      "Recall: 0.8875\n",
      "F1 Score: 0.7735041693765896\n",
      "AUC: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_accuracy.evaluate(prediction)\n",
    "precision = eval_precision.evaluate(prediction)\n",
    "recall = eval_recall.evaluate(prediction)\n",
    "f1score = eval_f1.evaluate(prediction)\n",
    "auc = eval_accuracy.evaluate(prediction)\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('F1 Score:',f1score)\n",
    "print('AUC:',auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|Outcome|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|       0.0|   19|\n",
      "|      0|       0.0|   68|\n",
      "|      1|       1.0|   24|\n",
      "|      0|       1.0|   12|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object and train on training data\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"Outcome\", maxIter=10).fit(df_train)\n",
    "# Make predictions.\n",
    "prediction = gbt.transform(df_test)\n",
    "# Select example rows to display.\n",
    "prediction.groupBy('Outcome', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7479674796747967\n",
      "Precision: 0.7816091954022989\n",
      "Recall: 0.85\n",
      "F1 Score: 0.7420836006283205\n",
      "AUC: 0.7479674796747967\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_accuracy.evaluate(prediction)\n",
    "precision = eval_precision.evaluate(prediction)\n",
    "recall = eval_recall.evaluate(prediction)\n",
    "f1score = eval_f1.evaluate(prediction)\n",
    "auc = eval_accuracy.evaluate(prediction)\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('F1 Score:',f1score)\n",
    "print('AUC:',auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|Outcome|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|       0.0|   24|\n",
      "|      0|       0.0|   55|\n",
      "|      1|       1.0|   19|\n",
      "|      0|       1.0|   25|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol=\"features\", labelCol=\"Outcome\").fit(df_train)\n",
    "\n",
    "# select example rows to display.\n",
    "prediction = nb.transform(df_test)\n",
    "# Select example rows to display.\n",
    "prediction.groupBy('Outcome', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6016260162601627\n",
      "Precision: 0.6962025316455697\n",
      "Recall: 0.6875\n",
      "F1 Score: 0.6026627735372994\n",
      "AUC: 0.6016260162601627\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_accuracy.evaluate(prediction)\n",
    "precision = eval_precision.evaluate(prediction)\n",
    "recall = eval_recall.evaluate(prediction)\n",
    "f1score = eval_f1.evaluate(prediction)\n",
    "auc = eval_accuracy.evaluate(prediction)\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('F1 Score:',f1score)\n",
    "print('AUC:',auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
